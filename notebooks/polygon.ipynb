{"cells":[{"cell_type":"code","source":["%python\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n          \"fs.azure.account.oauth2.client.id\": \"d0471621-cb16-4841-ab1b-e9c0fc3370ef\",\n          \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"azure-kv-scope\",key=\"sp-secret\"),\n          \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/25235581-c428-4ef3-a446-dddcdfada476/oauth2/token\"}\nif not dbutils.fs.ls(\"/mnt/\"):\n  dbutils.fs.mount(\n    source = \"abfss://bhoojal@bhoojalstorageaccount.dfs.core.windows.net/\",\n    mount_point = \"/mnt\",\n    extra_configs = configs)   \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"720163bd-20d8-49db-a9f4-3a8495740230"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%python\n%pip install geopandas\n%pip install azure-cosmos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7f22905-b791-44ad-80d5-7460b875e48f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import math\nimport pandas as pd\nimport json\nfrom shapely.geometry import Polygon\nfrom shapely.geometry.polygon import orient\nfrom pyspark.sql import functions as F\n\nimport geopandas as gpd\nfrom azure.cosmos import exceptions, CosmosClient, PartitionKey\nimport uuid\nfrom pyspark.sql import SparkSession\n\nendpoint = \"https://bhoojal-cosmos.documents.azure.com:443/\"\nkey = '0dDLs6jiq9oopPFt9oUkxx3Fima5EVJ6yEbPYIVAZY1oHqhyYxZ5mXcghgqsMvAabsVsBfi5Samh5V8onFc1NQ=='\nclient = CosmosClient(endpoint, key)\ndatabase_name = 'bhoojal_outlets'\ndatabase = client.create_database_if_not_exists(id=database_name)\n\ncontainer_depth = database.create_container_if_not_exists(\n    id=\"region_depth\", \n    partition_key=PartitionKey(path=\"/city\"),\n    offer_throughput=400\n)\n\nfileName = dbutils.widgets.get('FileName')\ngprData = spark.read.option(\"multiline\", \"true\").json(\"/mnt/raw/\" + fileName)\n\nfor row in gprData.collect():\n  geoJson = {}\n  depth = 0\n  geoJson['id'] = str(uuid.uuid4())\n  geoJson['scannedIn'] = row['scannedIn']\n  geoJson['city'] = row['city']\n  for observation in row['observations']:\n    rowDepth = (float(observation['waveTravelTime']) * 0.3)/(math.sqrt(float(row['dielectric'])))\n    depth = depth + rowDepth\n    \n  avgDepth = depth/len(row['observations'])\n  geoJson['depth'] = avgDepth\n    \n  res = [(float(val['lng']), float(val['lat'])) for val in row['observations']]\n  poly = orient(Polygon(res), sign=1.0)\n\n  pg=gpd.GeoDataFrame(index=[0], geometry=[poly])\n  jsonVal = json.loads(pg.to_json())\n    \n  for feature in jsonVal['features']:\n    geoJson['boundary'] = feature['geometry']\n  container_depth.create_item(body=geoJson)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07f4d979-a9f4-4a0c-a7ed-eeb87bc993dd"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"polygon","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3663200170526644}},"nbformat":4,"nbformat_minor":0}
